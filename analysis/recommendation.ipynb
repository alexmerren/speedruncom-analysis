{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems for Speedrun.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation by User ID using Bipartite Graph\n",
    "\n",
    "This method of recommendation works by finding other users that have played the same game as a target user. The games that the other users have played are ranked by occurrence. Games that the target user has not played but the other users have played are then recommended in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prefs_filename = \"../data/users/user_preferences_with_metadata.csv\"\n",
    "user_prefs_df = pd.read_csv(user_prefs_filename)\n",
    "user_prefs_df = user_prefs_df[(user_prefs_df['signup_date'].notna()) & (user_prefs_df['signup_date'] != \"Null\")]\n",
    "user_prefs_df['signup_date'] = pd.to_datetime(user_prefs_df['signup_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "user_prefs_df['signup_date'] = pd.to_datetime(user_prefs_df['signup_date'].dt.strftime('%Y-%m-%d'))\n",
    "user_prefs_df = user_prefs_df[(user_prefs_df['signup_date'] < '2023-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>location</th>\n",
       "      <th>num_games</th>\n",
       "      <th>games</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>335322</td>\n",
       "      <td>335322</td>\n",
       "      <td>335322</td>\n",
       "      <td>335322.000000</td>\n",
       "      <td>335322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>335322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>j5wzz2qj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>k6q4rqzd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-28 13:40:53.271780608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.994465</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-06 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-16 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-04 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-09-25 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2059.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.410692</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                    signup_date location      num_games  \\\n",
       "count     335322                         335322   335322  335322.000000   \n",
       "unique    335322                            NaN      257            NaN   \n",
       "top     j5wzz2qj                            NaN       us            NaN   \n",
       "freq           1                            NaN   101439            NaN   \n",
       "mean         NaN  2020-06-28 13:40:53.271780608      NaN       1.994465   \n",
       "min          NaN            2014-01-06 00:00:00      NaN       1.000000   \n",
       "25%          NaN            2019-09-16 00:00:00      NaN       1.000000   \n",
       "50%          NaN            2021-01-04 00:00:00      NaN       1.000000   \n",
       "75%          NaN            2021-09-25 00:00:00      NaN       2.000000   \n",
       "max          NaN            2022-12-31 00:00:00      NaN    2059.000000   \n",
       "std          NaN                            NaN      NaN       7.410692   \n",
       "\n",
       "           games  \n",
       "count     335322  \n",
       "unique     88806  \n",
       "top     k6q4rqzd  \n",
       "freq        5131  \n",
       "mean         NaN  \n",
       "min          NaN  \n",
       "25%          NaN  \n",
       "50%          NaN  \n",
       "75%          NaN  \n",
       "max          NaN  \n",
       "std          NaN  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prefs_df.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_games_df = user_prefs_df.copy()\n",
    "exploded_games_df['games'] = exploded_games_df['games'].str.split(',')\n",
    "exploded_games_df = exploded_games_df.explode('games').rename(columns = {'games': 'game_id', 'user':'user_id'})[['user_id', 'game_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>game_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>668788</td>\n",
       "      <td>668788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>335322</td>\n",
       "      <td>31425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>kj9521v8</td>\n",
       "      <td>k6q4rqzd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2059</td>\n",
       "      <td>6979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id   game_id\n",
       "count     668788    668788\n",
       "unique    335322     31425\n",
       "top     kj9521v8  k6q4rqzd\n",
       "freq        2059      6979"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_games_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite_graph = nx.Graph()\n",
    "\n",
    "# Users have a bipartite value of 0, games have a bipartite value of 1.\n",
    "bipartite_graph.add_nodes_from(set(exploded_games_df['user_id'].values), bipartite=0)\n",
    "bipartite_graph.add_nodes_from(set(exploded_games_df['game_id'].values), bipartite=1)\n",
    "bipartite_graph.add_edges_from([(user, game) for user, game in zip(exploded_games_df['user_id'], exploded_games_df['game_id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_bipartite(bipartite_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_prefs_filename, user_prefs_df, exploded_games_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlapping Set Similarity with Limiting the Graph\n",
    "\n",
    "There are two methods of limiting the number of user-item interactions in our bipartite graph. We can either use the mean and standard deviation of the `num_games` column, or limit based on the integer number of games played by a given user. For example, we can either use three standard deviations of the mean to have a cutoff value of `24.2 (3 s.f.)`, or we can use the value of `2` for users that have played only one game.\n",
    "\n",
    "Using the method of standard deviations, we get a very similar output to the unlimited user-item interaction bipartite graph. We get popular games recommended most of the time. If we use the second approach, we get (anecdotally) more precise recommendations. However, the second method does not scale well, since we need to construct a different graph for each number of games played by each user. In reality, this isn't as bad as we think. Out of the 335,322 total users in our sample we can cover 306,371 users, or 91.4% (3 s.f.) of them with three graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_user_preferences_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[(df['signup_date'].notna()) & (df['signup_date'] != \"Null\")]\n",
    "    df['signup_date'] = pd.to_datetime(df['signup_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "    df['signup_date'] = pd.to_datetime(df['signup_date'].dt.strftime('%Y-%m-%d'))\n",
    "    df = df[(df['signup_date'] < '2023-01-01')]\n",
    "    return df\n",
    "\n",
    "def limit_number_games_user_preferences_df(df: pd.DataFrame, num_games: int) -> pd.DataFrame:\n",
    "    return df[(df['num_games'] <= num_games)]\n",
    "\n",
    "def explode_games_played(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['games'] = df['games'].str.split(',')\n",
    "    return df.explode('games').rename(columns = {'games': 'game_id', 'user':'user_id'})\n",
    "\n",
    "def recommendation_graph_for_n_games_played(df: pd.DataFrame, n: int) -> tuple[pd.DataFrame, nx.Graph]:\n",
    "    df = clean_user_preferences_df(df)\n",
    "    df = limit_number_games_user_preferences_df(df, n+1)\n",
    "    df = explode_games_played(df)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    bipartite_graph.add_nodes_from(set(df['user_id'].values), bipartite=0)\n",
    "    bipartite_graph.add_nodes_from(set(df['game_id'].values), bipartite=1)\n",
    "    bipartite_graph.add_edges_from([(user, game) for user, game in zip(df['user_id'], df['game_id'])])\n",
    "    return df, bipartite_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/pwtrzcy130zdfcx62mp9_kmc0000gn/T/ipykernel_2763/4036755379.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['signup_date'] = pd.to_datetime(df['signup_date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
      "/var/folders/fp/pwtrzcy130zdfcx62mp9_kmc0000gn/T/ipykernel_2763/4036755379.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['signup_date'] = pd.to_datetime(df['signup_date'].dt.strftime('%Y-%m-%d'))\n"
     ]
    }
   ],
   "source": [
    "user_prefs_df = pd.read_csv('../data/users/user_preferences_with_metadata.csv')\n",
    "user_prefs_df, bipartite_graph = recommendation_graph_for_n_games_played(user_prefs_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_similarity(bipartite_graph: nx.Graph, total_item_nodes: int, user_a_id: str, user_b_id: str) -> float:\n",
    "    assert bipartite_graph.nodes[user_a_id]['bipartite'] == 0\n",
    "    assert bipartite_graph.nodes[user_b_id]['bipartite'] == 0\n",
    "\n",
    "    a_neighbours = bipartite_graph.neighbors(user_a_id)\n",
    "    b_neighbours = bipartite_graph.neighbors(user_b_id)\n",
    "    shared_nodes = set(a_neighbours).intersection(b_neighbours)\n",
    "\n",
    "    return len(shared_nodes) / total_item_nodes\n",
    "\n",
    "def most_similar_users(bipartite_graph: nx.Graph, user_id: str) -> tuple[list[str], float]:\n",
    "    all_users = set([user for user, value in bipartite_graph.nodes(data=True) if value['bipartite'] == 0])\n",
    "    all_users.remove(user_id)\n",
    "\n",
    "    total_item_nodes = 0\n",
    "    for _, values in bipartite_graph.nodes(data=True):\n",
    "        if values['bipartite'] == 1: total_item_nodes += 1\n",
    "\n",
    "    similarities = defaultdict(float)\n",
    "    for user in all_users:\n",
    "        similarities[user] = user_similarity(bipartite_graph, total_item_nodes, user_id, user)\n",
    "\n",
    "    max_similarity = max(similarities.values())\n",
    "    return [user for user, similarity in similarities.items() if similarity == max_similarity], max_similarity\n",
    "\n",
    "def recommend_games(bipartite_graph: nx.Graph, user_id: str) -> list[str]:\n",
    "    similar_users, _ = most_similar_users(bipartite_graph, user_id)\n",
    "    other_games = [game for user in similar_users for game in bipartite_graph.neighbors(user)]\n",
    "    game_rankings = Counter(other_games)\n",
    "\n",
    "    already_played_games = set(bipartite_graph.neighbors(user_id))\n",
    "\n",
    "    try:\n",
    "        [game_rankings.pop(game) for game in already_played_games]\n",
    "    except KeyError:\n",
    "        # If no other users in data set have played this game.\n",
    "        pass\n",
    "\n",
    "    ranked_games_in_order, _ = list(zip(*sorted(game_rankings.items(), key=itemgetter(1), reverse=True)))\n",
    "    \n",
    "    return ranked_games_in_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_metadata_df = pd.read_csv('../data/games/metadata/all_games.csv').rename(columns={'game_id': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played games: ['Hello Neighbor' 'Super Mario Odyssey']\n",
      "Recommended games: [\"Baldi's Basics Category Extensions\" 'Google Quick Draw'\n",
      " 'Snipperclips: Cut it out  together!' 'Hello Neighbor 2' 'Cuphead'\n",
      " 'Super Mario Sunshine' 'Marble Saga: Kororinpa'\n",
      " 'The Legend of Zelda: The Wind Waker HD' 'Clicker Heroes'\n",
      " 'Minecraft: Java Edition' 'Island Saver'\n",
      " 'Super Mario Odyssey Category Extensions']\n"
     ]
    }
   ],
   "source": [
    "user = \"x355n6qj\"\n",
    "\n",
    "played_games = list(bipartite_graph.neighbors(user))\n",
    "print(f\"Played games: {games_metadata_df[(games_metadata_df['id'].isin(played_games))].game_name.values}\")\n",
    "\n",
    "recommended_games = recommend_games(bipartite_graph, user)\n",
    "print(f\"Recommended games: {games_metadata_df.set_index('id').loc(axis=0)[recommended_games].game_name.values[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "del games_metadata_df, played_games, recommended_games, user_prefs_df, bipartite_graph, user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Using a Game Similarity Matrix\n",
    "\n",
    "This recommendation method works by creating a matrix of how users have rated different games. We construct this matrix by rating a game `1` if a user has played it, and `0` if not. We then normalise these values by making the sum of ratings by each user equal to `1`. This is also called [scaling to a unit length](https://en.wikipedia.org/wiki/Feature_scaling#Scaling_to_unit_length). We take the dot product of the matrix and the transposed matrix, and this gives us the similarity between each item in the data set. The method is taken from [here](https://towardsdatascience.com/recommender-systems-item-customer-collaborative-filtering-ff0c8f41ae8a). **This method does not scale very well**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prefs_df = pd.read_csv('../data/users/user_preferences_with_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_user_prefs_df_to_ratings(df: pd.DataFrame, number_users=-1) -> pd.DataFrame:\n",
    "    tmp_df = df.copy().sample(number_users)\n",
    "    tmp_df['games'] = tmp_df['games'].str.split(',')\n",
    "    tmp_df = tmp_df.explode('games').rename(columns = {'games': 'game_id', 'user':'user_id'})\n",
    "    tmp_df['rating'] = 1\n",
    "    return tmp_df[['user_id', 'game_id', 'rating']]\n",
    "\n",
    "def construct_similarity_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df = pd.pivot_table(tmp_df, values='rating', index='user_id', columns='game_id')\n",
    "    tmp_df = tmp_df.fillna(0.0)\n",
    "    normalised_tmp_df = tmp_df / np.sqrt(np.square(tmp_df).sum(axis=0))\n",
    "    return normalised_tmp_df.transpose().dot(normalised_tmp_df)\n",
    "\n",
    "def recommend_from_game(similarity_matrix: pd.DataFrame, game_id: str, n_recommendations: int) -> list[str]:\n",
    "    return similarity_matrix.nlargest(n_recommendations+1, game_id).index.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\ajcm203\\speedruncom-data\\analysis\\recommendation.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ajcm203/speedruncom-data/analysis/recommendation.ipynb#ch0000020?line=6'>7</a>\u001b[0m     similarity_matrix\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m./test.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ajcm203/speedruncom-data/analysis/recommendation.ipynb#ch0000020?line=7'>8</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ajcm203/speedruncom-data/analysis/recommendation.ipynb#ch0000020?line=8'>9</a>\u001b[0m     similarity_matrix \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../data/users/game_similarity_matrix.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=579'>580</a>\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=580'>581</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=1251'>1252</a>\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=1252'>1253</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=1253'>1254</a>\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=1254'>1255</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/readers.py?line=1255'>1256</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=222'>223</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=223'>224</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=224'>225</a>\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=225'>226</a>\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=226'>227</a>\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1147\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1429\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1419'>1420</a>\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1420'>1421</a>\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1421'>1422</a>\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1422'>1423</a>\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1423'>1424</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1424'>1425</a>\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1425'>1426</a>\u001b[0m     )\n\u001b[1;32m-> <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1428'>1429</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1429'>1430</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1430'>1431</a>\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1431'>1432</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1471'>1472</a>\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1472'>1473</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Anaconda/lib/site-packages/pandas/core/dtypes/common.py?line=1473'>1474</a>\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "similarity_matrix = pd.DataFrame()\n",
    "generate = False\n",
    "\n",
    "if generate:\n",
    "    ratings_df = format_user_prefs_df_to_ratings(user_prefs_df, number_users=5000)\n",
    "    similarity_matrix = construct_similarity_matrix(ratings_df)\n",
    "    similarity_matrix.to_csv('./test.csv')\n",
    "else:\n",
    "    similarity_matrix = pd.read_csv('../data/users/game_similarity_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = similarity_matrix.set_index('game_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_metadata_df = pd.read_csv('../data/games/metadata/all_games.csv').rename(columns={'game_id': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played Game: LEGO Harry Potter: Years 5-7 (DS/PSP)\n",
      "Recommended games: ['LEGO Indiana Jones 2: The Adventure Continues (DS)'\n",
      " 'LEGO Star Wars II: The Original Trilogy (PSP)' 'Escape PS1 Hagrid'\n",
      " 'LEGO Star Wars III: The Clone Wars (DS)'\n",
      " 'LEGO Pirates of the Caribbean: The Video Game (DS)']\n"
     ]
    }
   ],
   "source": [
    "game_id = \"kdkmzmx1\"\n",
    "n = 5\n",
    "recommended_games = recommend_from_game(similarity_matrix, game_id, n)\n",
    "\n",
    "print(f\"Played Game: {games_metadata_df.set_index('id').loc(axis=0)[game_id].game_name}\")\n",
    "print(f\"Recommended games: {games_metadata_df.set_index('id').loc(axis=0)[recommended_games].game_name.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del game_id, generate, recommended_games, user_prefs_df, games_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of Cosine Similarity CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_user_prefs_df_to_ratings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df['games'] = tmp_df['games'].str.split(',')\n",
    "    tmp_df = tmp_df.explode('games').rename(columns = {'games': 'game_id', 'user':'user_id'})\n",
    "    tmp_df['rating'] = 1\n",
    "    return tmp_df[['user_id', 'game_id', 'rating']]\n",
    "\n",
    "def construct_similarity_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df = pd.pivot_table(tmp_df, values='rating', index='user_id', columns='game_id')\n",
    "    tmp_df = tmp_df.fillna(0.0)\n",
    "    normalised_tmp_df = tmp_df / np.sqrt(np.square(tmp_df).sum(axis=0))\n",
    "    return normalised_tmp_df.transpose().dot(normalised_tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prefs_df = pd.read_csv('../data/users/user_preferences_with_metadata.csv').sample(5000, random_state=0)\n",
    "# user_prefs_df = pd.read_csv('../data/users/user_preferences_with_metadata.csv').sort_values(by='num_games', ascending=True)[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(user_prefs_df, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create cosine similarity matrix for test data, create recommendation engine using that cosine similarity matrix. \n",
    "train_df = format_user_prefs_df_to_ratings(train_df)\n",
    "similarity_matrix_t = construct_similarity_matrix(train_df)\n",
    "\n",
    "# Find the games played by the test users. Only use them if they play more than one game.\n",
    "recommendations = []\n",
    "unique_games_in_test = np.unique(test_df['games'].str.split(',').tolist()[0])\n",
    "for game in unique_games_in_test:\n",
    "    game_row = similarity_matrix_t.loc[game]\n",
    "    recs = game_row[(game_row > 0.5).any() & (game_row != 1.0)].index.values.tolist()[0]\n",
    "    recommendations.append(recs)\n",
    "\n",
    "relevant_games_set = [set(games) for games in unique_games_in_test]\n",
    "recommended_games_set = [set(games) for games in recommendations]\n",
    "intersection = [len(recommended.intersection(relevant)) for recommended, relevant in zip(recommended_games_set, relevant_games_set)]\n",
    "recall = np.mean([i/len(relevant) for i, relevant in zip(intersection, relevant_games_set)])\n",
    "precision = np.mean([i/len(recommended) for i, recommended in zip(intersection, recommended_games_set)])\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.42857142857142855\n",
      "Precision: 0.375\n",
      "F1-score: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1-score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation using Node2Vec Embeddings\n",
    "\n",
    "The idea behind using node2vec embeddings for recommendation is to predict future links that don't already exist. We can prove that this works for individual games recommendation by removing selected edges and using cosine similarity of embeddings to predict which edges should exist given this graph. We carry this on further by creating a pipeline to predict games to play when they are completely disconnected.\n",
    "\n",
    "Do [this](https://sparsh-ai.github.io/rec-tutorials/graph%20embedding%20movielens%20factorization/2021/04/24/Recommendation-Node2vec.html)?\n",
    "\n",
    "This guy ^ goes absolutely fucking crazy. he's got tonnes of stuff. This one looks goated: https://github.com/sparsh-ai?tab=repositories&q=rec-tut&type=&language=&sort=. Specifically the movielens100k dataset, but all of them probably have some really good stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, auc, roc_curve, roc_auc_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_existence_df = pd.read_csv('../data/games/network/edge_existence_dataset.csv').set_index('index')\n",
    "embeddings_df = pd.read_csv('../data/games/network/test.emb', delimiter=\" \", skiprows=1, index_col=0, header=None).rename(columns={0: 'id'}).add_prefix(\"dim_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features = [(embeddings_df.loc[node1].values.tolist()+embeddings_df.loc[node2].values.tolist()) for node1, node2 in zip(edge_existence_df[\"source\"], edge_existence_df[\"target\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(edge_features)\n",
    "y = edge_existence_df['connection']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10471, 32), (10471,))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid best parameter (max. accuracy):  {'max_depth': 15, 'n_estimators': 50}\n",
      "Grid best score (accuracy):  0.9985675631933949\n",
      "Test set AUC:  0.999998330779426\n",
      "Grid best parameter (max. AUC):  {'max_depth': 10, 'n_estimators': 50}\n",
      "Grid best score (AUC):  0.9999765448696089\n"
     ]
    }
   ],
   "source": [
    "clf1 = RandomForestClassifier()\n",
    " \n",
    "# parameters\n",
    "param = {'n_estimators' : [10,50,100], 'max_depth' : [5,10,15]}\n",
    " \n",
    "# model\n",
    "grid_clf_acc1 = GridSearchCV(clf1, param_grid = param)\n",
    " \n",
    "# train the model\n",
    "grid_clf_acc1.fit(X_train, y_train)\n",
    " \n",
    "print('Grid best parameter (max. accuracy): ', grid_clf_acc1.best_params_)\n",
    "print('Grid best score (accuracy): ', grid_clf_acc1.best_score_)\n",
    " \n",
    "# alternative metric to optimize over grid parameters: AUC\n",
    "grid_clf_auc1 = GridSearchCV(clf1, param_grid = param, scoring = 'roc_auc')\n",
    "grid_clf_auc1.fit(X_train, y_train)\n",
    "predict_proba = grid_clf_auc1.predict_proba(X_test)[:,1]\n",
    " \n",
    "print('Test set AUC: ', roc_auc_score(y_test, predict_proba))\n",
    "print('Grid best parameter (max. AUC): ', grid_clf_auc1.best_params_)\n",
    "print('Grid best score (AUC): ', grid_clf_auc1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 929,    0],\n",
       "       [  10, 2552]], dtype=int64)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_clf_auc1.predict(X_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 64 features, but RandomForestClassifier is expecting 32 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\ajcm203\\speedruncom-data\\analysis\\recommendation.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ajcm203/speedruncom-data/analysis/recommendation.ipynb#ch0000056?line=3'>4</a>\u001b[0m SM64 \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m0.012588676\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.029154047\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.0028756857\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.009487752\u001b[39m, \u001b[39m0.017457027\u001b[39m, \u001b[39m0.0210394\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.010649294\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.0018646903\u001b[39m, \u001b[39m0.022166133\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.024311021\u001b[39m, \u001b[39m0.0062766\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.029152796\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.030749485\u001b[39m, \u001b[39m0.0017411523\u001b[39m, \u001b[39m0.029207725\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.015724432\u001b[39m, \u001b[39m0.009961858\u001b[39m, \u001b[39m0.008669887\u001b[39m, \u001b[39m0.01067755\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.012766957\u001b[39m, \u001b[39m0.0050436445\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.0035253912\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.008545488\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.022413097\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.014777329\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.005886156\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.024985388\u001b[39m, \u001b[39m0.004289292\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.0025947131\u001b[39m, \u001b[39m0.0019044504\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.014002968\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.02486419\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ajcm203/speedruncom-data/analysis/recommendation.ipynb#ch0000056?line=5'>6</a>\u001b[0m smo_sm64_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(SMO \u001b[39m+\u001b[39m SM64)\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ajcm203/speedruncom-data/analysis/recommendation.ipynb#ch0000056?line=6'>7</a>\u001b[0m grid_clf_auc1\u001b[39m.\u001b[39;49mpredict(smo_sm64_features)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/utils/metaestimators.py?line=109'>110</a>\u001b[0m         \u001b[39mraise\u001b[39;00m attr_err\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/utils/metaestimators.py?line=111'>112</a>\u001b[0m     \u001b[39m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/utils/metaestimators.py?line=112'>113</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/utils/metaestimators.py?line=113'>114</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/utils/metaestimators.py?line=115'>116</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:521\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/model_selection/_search.py?line=502'>503</a>\u001b[0m \u001b[39m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/model_selection/_search.py?line=503'>504</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/model_selection/_search.py?line=504'>505</a>\u001b[0m \u001b[39mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/model_selection/_search.py?line=517'>518</a>\u001b[0m \u001b[39m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/model_selection/_search.py?line=518'>519</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/model_selection/_search.py?line=519'>520</a>\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/model_selection/_search.py?line=520'>521</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mpredict(X)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:808\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=786'>787</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=787'>788</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=788'>789</a>\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=789'>790</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=805'>806</a>\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=806'>807</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=807'>808</a>\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=809'>810</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=810'>811</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:850\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=847'>848</a>\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=848'>849</a>\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=849'>850</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=851'>852</a>\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=852'>853</a>\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:579\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=575'>576</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=576'>577</a>\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=577'>578</a>\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=578'>579</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=579'>580</a>\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/ensemble/_forest.py?line=580'>581</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=581'>582</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=583'>584</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=584'>585</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=586'>587</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=396'>397</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=398'>399</a>\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=399'>400</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=400'>401</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=401'>402</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Anaconda/lib/site-packages/sklearn/base.py?line=402'>403</a>\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 64 features, but RandomForestClassifier is expecting 32 features as input."
     ]
    }
   ],
   "source": [
    "# Test if there is a link between super mario odyssey and super mario 64 (there is).\n",
    "# IDs: 76r55vd8 o1y9wo6q\n",
    "SMO = [0.003971752, 0.00646716, 0.005253054, 0.000770621, 0.023088027, -0.0025586337, 0.021919776, 0.023069408, 0.00055524334, 0.018905181, -0.015266668, -0.0029585548, 0.010636453, -0.017556641, -0.0020937473, -0.029378641, 0.013062697, -0.009451449, -0.020212192, 0.017371856, 0.003602054, -0.026192848, -0.025948, 0.017900482, 0.02082228, -0.019447599, 0.020568952, 0.025263663, -0.01999519, -0.023548912, -0.019287951, -0.011146348]\n",
    "SM64 = [-0.012588676, -0.029154047, -0.0028756857, -0.009487752, 0.017457027, 0.0210394, -0.010649294, -0.0018646903, 0.022166133, -0.024311021, 0.0062766, -0.029152796, -0.030749485, 0.0017411523, 0.029207725, -0.015724432, 0.009961858, 0.008669887, 0.01067755, -0.012766957, 0.0050436445, -0.0035253912, -0.008545488, -0.022413097, -0.014777329, -0.005886156, -0.024985388, 0.004289292, -0.0025947131, 0.0019044504, -0.014002968, -0.02486419]\n",
    "\n",
    "smo_sm64_features = np.array(SMO + SM64).reshape(1, -1)\n",
    "grid_clf_auc1.predict(smo_sm64_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uhoh"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed225720166559e7176d3793db16a2fd8d295f725007103b21ac3099d2a89ee8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
